import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import os

from pydataset import data
from scipy import stats
from env import get_connection
# import splitting and imputing functions
from sklearn.model_selection import train_test_split
from sklearn.impute import SimpleImputer

# turn off pink boxes forb demo
import warnings
warnings.filterwarnings("ignore")
# import our own acquire module
import acquire


titanic_df = acquire.get_titanic_data()


# rows & columns
titanic_df.shape


# first n rows
titanic_df.head(2)


# Get information about the dataframe: column names, rows, datatypes, non-missing values.
titanic_df.info()


# Get summary statistics for numeric columns.
titanic_df.describe()


# Check out distributions of numeric columns.

num_cols = titanic_df.columns[[titanic_df[col].dtype == 'int64' for col in titanic_df.columns]]
for col in num_cols:
    plt.hist(titanic_df[col])
    plt.title(col)
    plt.show()



# Use .describe with object columns.

obj_cols = titanic_df.columns[[titanic_df[col].dtype == 'O' for col in titanic_df.columns]]
for col in obj_cols:
    print(titanic_df[col].value_counts())
    print(titanic_df[col].value_counts(normalize=True, dropna=False))
    print('-------------------')


# Create bins for fare using /value_counts
# Using sort = false will sort by bin values as opposed to the frequency counts

titanic_df.fare.value_counts(bins=5, sort=False)


# Find columns with missing values and the total of missing values.

missing = titanic_df.isnull().sum()
missing[missing > 0]


# Drop duplicates...run just incase; reassign and check the shape of my datal.

titanic_df = titanic_df.drop_duplicates()
titanic_df.shape


# Drop columns with too many missing value for now and reassign; check the shape

cols_to_drop = ['deck', 'embarked', 'class', 'age']
titanic_df = titanic_df.drop(columns=cols_to_drop)
titanic_df.shape


# validate that the colummns are dropped.

titanic_df.head(1)


# Run .fillna() on the entire tianic_df

titanic_df['embark_town'] = titanic_df.embark_town.fillna(value='Southhampton')


# validate that missing values in embark_town have been modified

titanic_df.embark_town.isna().sum()


# Using drop_first leaves sex_male, embark_town_Queenstown, and embark_town_Southampton.

dummy_df = pd.get_dummies(titanic_df[['sex', 'embark_town']], dummy_na=False, drop_first=[True, True])
dummy_df.head()


#Concatenate the dummy_df dataframe above with the original df and validate.

titanic_df = pd.concat([titanic_df, dummy_df], axis=1)
titanic_df.head()


# Create a function that will automate these steps for when we need to reproduces our process

def clean_data(titanic_df):
    '''
    This function will drop any duplicate observations,
    drop ['deck', embarked', 'class', 'age'], fill missing embark_town with 'Southampton'
    and create summy var from sex and embark town
    '''
    titanic_df = titanic_df.drop_duplicates()
    titanic_df = titanic_df.drop(columns=['deck', 'embarked', 'class', 'age'])
    titanic_df['embark_town'] = titanic_df.embark_town.fillna(value='Southampton')
    dummy_df = pd.get_dummies(titanic_df[['sex', 'embark_town']], drop_first=True)
    titanic_df = pd.concat([titanic_df, dummy_df], axis=1)
    return titanic_df


df = acquire.get_titanic_data()
df.head()


df = clean_data(df)
df.head


df.info()


# 20% test, 80% train_validate
# then of the 80% train_validate: 30% validate, 70% train.

train, test = train_test_split(df, test_size=.2, random_state=123, stratify=df.survived)
train, validate = train_test_split(train, test_size=.3, random_state=123, stratify=train.survived)



# Validate my split.

print(f'train -> {train.shape}')
print(f'validate -> {validate.shape}')
print(f'test -> {test.shape}')


def split_data(df):
    '''
    take in a DataFrame and return train, validate, and test DataFrames; stratify on survived.
    return train, validate, test DataFrames.
    '''
    train_validate, test = train_test_split(df, test_size=.2, random_state=123, stratify=df.survived)
    train, validate = train_test_split(train_validate, 
                                       test_size=.3, 
                                       random_state=123, 
                                       stratify=train_validate.survived)
    return train, validate, test


train, validate, test = split_data(df)


# Validate my split

print(f'train -> {train.shape}')
print(f'validate -> {validate.shape}')
print(f'test -> {test.shape}')


# Get fresh Titanic data to use with missing values in embark_town again.

df = acquire.get_titanic_data()
train, validate, test = split_data(df)


# ONLY look at train dataset after we split our data.

train.info()



missing_rows = train[train['embark_town'].isna()]
print(missing_rows)


'''
- Create the SimpleImputer object, which we will store in the variable imputer. In the creation of the object, 
we will specify the strategy to use (mean, median, most_frequent). Essentially, this is creating the instructions 
and assigning them to a variable, imputer.

- Fit the imputer to the columns in the training df. This means that the imputer will determine the most_frequent value, 
or other value depending on the strategy called, for each column.
'''

imputer = SimpleImputer(strategy='most_frequent')

# this scales very well, example if you wanted to impute for 50 columns for example, it would find the strategy assigned and do it for each column and return that assigned strategy for each column


imputer = imputer.fit(train[['embark_town']])


train[['embark_town']] = imputer.transform(train[['embark_town']])

validate[['embark_town']] = imputer.transform(validate[['embark_town']])

test[['embark_town']] = imputer.transform(test[['embark_town']])



# Validate that there are no longer any Null values in embark_town.

train.embark_town.value_counts(dropna=False)


def impute_mode(train, validate, test):
    '''
    take in train, validate, and test DataFrames, impute mode for embark_town,
    and return train, validate, and test DataFrames
    '''
    imputer = SimpleImputer(missing_values = None, strategy='most_frequent')
    train[['embark_town']] = imputer.fit_transform(train[['embark_town']])
    validate[['embark_town']] = imputer.transform(validate[['embark_town']])
    test[['embark_town']] = imputer.transform(test[['embark_town']])
    return train, validate, test


'''
Use our helper functions clean_data() and split_data() in our single prep_titanic_data() function 
that takes in my Titanic DataFrame and returns cleaned and split datasets with the mode value 
imputed for missing values in embark_town.
'''
# Acquire fresh Titanic data to test my function.

df = acquire.get_titanic_data()


def prep_titanic_data(df):
    '''
    This function takes in a df and will drop any duplicate observations, 
    drop ['deck', 'embarked', 'class', 'age'], fill missing embark_town with 'Southampton'
    create dummy vars from sex and embark_town, and perform a train, validate, test split. 
    Returns train, validate, and test DataFrames
    '''
    df = clean_data(df)
    train, validate, test = split_data(df)
    return train, validate, test


# Run final prepare function and validate what that the function is working properly.

train, validate, test = prep_titanic_data(df)
train.info()

