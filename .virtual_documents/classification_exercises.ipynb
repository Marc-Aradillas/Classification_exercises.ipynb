








import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from pydataset import data
from scipy import stats
from env import get_connection


print(sns.get_dataset_names())








df_iris= sns.load_dataset('iris')
df_iris.info()


# this is how to display data from pydataset lib
data('iris').head()


df_iris.head(3)


df_iris.shape


df_iris.columns


df_iris.dtypes


df_iris.describe().T








url = 'https://docs.google.com/spreadsheets/d/1Uhtml8KY19LILuZsrDtlsHHDC9wuDGUSe8LTEwvdI5g/edit?usp=sharing'

export_url = url.replace('/edit?usp=sharing', '/export?format=csv')

df_google = pd.read_csv(export_url)


df_google.head(3)


df_google.shape


df_google.columns.to_list()


df_google.dtypes


df_google.describe().T


df_google.nunique()





# I could get a count of unique values for each variable with an object data type.
for col in df_google.columns:
#     print(col)
    if df_google[col].dtypes == 'object':
        print(f'{col} has {df_google[col].nunique()} unique values.')








df_excel = pd.read_excel('train.xlsx')
df_excel.head()


df_excel_sample = df_excel[:100]
df_excel_sample.head()


df_excel.shape[0]


df_excel.columns[:5]


df_excel.columns.astype('object')


df_excel.dtypes


# Age range
df_excel.Age.max() - df_excel.Age.min()


# Fare range
df_excel.Fare.max() - df_excel.Fare.min()





import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import os
import acquire

from sklearn.impute import SimpleImputer
from prepare import train_val_test
from sklearn.model_selection import train_test_split
from scipy import stats
from env import get_connection




















iris_df = acquire.get_iris_data()


iris_df.info() 


iris_df.head()


i_df = iris_df.drop(columns=['species_id', 'species_id.1', 'measurement_id'])
i_df.head()


i_df = i_df.rename(columns={'species_name' : 'species'})
i_df.head()





'''
To create dummy variables for the 'species' column in the Iris dataset 
and concatenate them onto the existing dataframe, we can use the pandas 
library in Python. 
'''
import pandas as pd
from sklearn.datasets import load_iris

# Load the Iris dataset
iris = load_iris()
iris_df = pd.DataFrame(data=iris.data, columns=iris.feature_names)

# Add the 'species' column to the dataframe
iris_df['species'] = iris.target_names[iris.target]

# Create dummy variables for the 'species' column
species_dummies = pd.get_dummies(iris_df['species'], prefix='species')

# Concatenate the dummy variables onto the original dataframe
iris_df = pd.concat([iris_df, species_dummies], axis=1)

# Display the modified dataframe
print(iris_df.head())





def prep_iris():

    iris_df = acquire.get_iris_data()

    i_df = iris_df.drop(columns=['species_id', 'species_id.1', 'measurement_id'])

    i_df = i_df.rename(columns={'species_name' : 'species'})

    return i_df
    


prep_iris().head()


i_df = prep_iris()








titanic_data = acquire.get_titanic_data()
titanic_data.info()


titanic_data.isna().sum() # brief statement tells me how many nulls i have and in which column


round(titanic_data.isna().sum() /len(titanic_data) * 100, 2)


titanic_data.head()


t_data = titanic_data.drop(columns= ['embarked', 'parch'])


round(titanic_data.isna().sum() /len(titanic_data) * 100, 2)


def prep_titanic():

    titanic_data = acquire.get_titanic_data()
    
    t_data = titanic_data.drop(columns= ['embarked', 'parch'])

    return t_data


t_data.info()


prep_titanic().head()


t_df = prep_titanic()








telco_data = acquire.get_telco_data()
telco_data.info()


telco_data.isna().sum() # brief statement tells me how many nulls i have and in which column


te_data = telco_data.drop(columns=['customer_id', 'partner', 'payment_type_id', 'contract_type_id', 'internet_service_type_id'])
te_data.head()


def prep_telco():

    telco_data = acquire.get_telco_data()

    te_data = telco_data.drop(columns=['customer_id', 'payment_type_id', 'contract_type_id', 'internet_service_type_id'])

    return te_data


prep_telco().head()


te_df = prep_telco()


te_df.info()


te_df.internet_service_type.unique








# this function will be good for splitting classification data per our target

def train_val_test(df, strat, seed = 42):

    train, val_test = train_test_split(df, train_size = 0.7,
                                       random_state = seed,
                                       stratify = df[strat])

    val, test = train_test_split(val_test, train_size = 0.5,
                                 random_state = seed,
                                 stratify = val_test[strat])

    return train, val, test


train_iris, validate_iris, test_iris = train_val_test(i_df, strat='species')


# Validate my split

print(f'train_iris -> {train_iris.shape}')
print(f'validate_iris -> {validate_iris.shape}')
print(f'test_iris -> {test_iris.shape}')


train_titanic, validate_titanic, test_titanic = train_val_test(t_df, strat='survived')


# Validate my split

print(f'train_titanic -> {train_titanic.shape}')
print(f'validate_titanic -> {validate_titanic.shape}')
print(f'test_titanic -> {test_titanic.shape}')


train_telco, validate_telco, test_telco = train_val_test(te_df, strat='churn')


# Validate my split

print(f'train_telco -> {train_telco.shape}')
print(f'validate_telco -> {validate_telco.shape}')
print(f'test_telco -> {test_telco.shape}')








# 20% test, 80% train_validate
# then of the 80% train_validate: 30% validate, 70% train.

train, test = train_test_split(i_df, test_size=.2, random_state=123, stratify=i_df.species)
train, validate = train_test_split(train, test_size=.3, random_state=123, stratify=train.species)



# Validate my split.

print(f'train -> {train.shape}')
print(f'validate -> {validate.shape}')
print(f'test -> {test.shape}')


def split_data(df):
    '''
    take in a DataFrame and return train, validate, and test DataFrames; stratify on survived.
    return train, validate, test DataFrames.
    '''
    train_validate, test = train_test_split(df, test_size=.2, random_state=123, stratify=df.species)
    train, validate = train_test_split(train_validate, 
                                       test_size=.3, 
                                       random_state=123, 
                                       stratify=train_validate.species)
    
    return train, validate, test


train_i, validate_i, test_i = split_data(i_df)


# Validate my split

print(f'train_iris -> {train_iris.shape}')
print(f'validate_iris -> {validate_iris.shape}')
print(f'test_iris -> {test_iris.shape}')








# 20% test, 80% train_validate
# then of the 80% train_validate: 30% validate, 70% train.

train, test = train_test_split(t_df, test_size=.2, random_state=123, stratify=t_df.survived)
train, validate = train_test_split(train, test_size=.3, random_state=123, stratify=train.survived)



# Validate my split.

print(f'train_titanic -> {train.shape}')
print(f'validate_titanic -> {validate.shape}')
print(f'test_titanic -> {test.shape}')


def split_data(df):
    '''
    take in a DataFrame and return train, validate, and test DataFrames; stratify on survived.
    return train, validate, test DataFrames.
    '''
    train_validate, test = train_test_split(t_df, test_size=.2, random_state=123, stratify=t_df.survived)
    train, validate = train_test_split(train_validate, 
                                       test_size=.3, 
                                       random_state=123, 
                                       stratify=train_validate.survived)
    
    return train, validate, test


train_t, validate_t, test_t = split_data(t_df)


# Validate my split.

print(f'train_titanic -> {train.shape}')
print(f'validate_titanic -> {validate.shape}')
print(f'test_titanic -> {test.shape}')








# 20% test, 80% train_validate
# then of the 80% train_validate: 30% validate, 70% train.

train, test = train_test_split(te_df, test_size=.2, random_state=123, stratify=te_df.churn)
train, validate = train_test_split(train, test_size=.3, random_state=123, stratify=train.churn)



# Validate my split.

print(f'train_telco -> {train.shape}')
print(f'validate_telco -> {validate.shape}')
print(f'test_telco -> {test.shape}')


def split_data(df):
    '''
    take in a DataFrame and return train, validate, and test DataFrames; stratify on survived.
    return train, validate, test DataFrames.
    '''
    train_validate, test = train_test_split(te_df, test_size=.2, random_state=123, stratify=te_df.churn)
    train, validate = train_test_split(train_validate, 
                                       test_size=.3, 
                                       random_state=123, 
                                       stratify=train_validate.churn)
    
    return train, validate, test


train_te, validate_te, test_te = split_data(te_df)


# Validate my split.

print(f'train_telco -> {train.shape}')
print(f'validate_telco -> {validate.shape}')
print(f'test_telco -> {test.shape}')






























