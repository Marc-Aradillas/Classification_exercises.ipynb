














import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from pydataset import data
from scipy import stats
from env import get_connection


print(sns.get_dataset_names())








df_iris= sns.load_dataset('iris')
df_iris.info()


# this is how to display data from pydataset lib
data('iris').head()


df_iris.head(3)


df_iris.shape


df_iris.columns


df_iris.dtypes


df_iris.describe().T








url = 'https://docs.google.com/spreadsheets/d/1Uhtml8KY19LILuZsrDtlsHHDC9wuDGUSe8LTEwvdI5g/edit?usp=sharing'

export_url = url.replace('/edit?usp=sharing', '/export?format=csv')

df_google = pd.read_csv(export_url)


df_google.head(3)


df_google.shape


df_google.columns.to_list()


df_google.dtypes


df_google.describe().T


df_google.nunique()





# I could get a count of unique values for each variable with an object data type.
for col in df_google.columns:
#     print(col)
    if df_google[col].dtypes == 'object':
        print(f'{col} has {df_google[col].nunique()} unique values.')








df_excel = pd.read_excel('train.xlsx')
df_excel.head()


df_excel_sample = df_excel[:100]
df_excel_sample.head()


df_excel.shape[0]


df_excel.columns[:5]


df_excel.columns.astype('object')


df_excel.dtypes


# Age range
df_excel.Age.max() - df_excel.Age.min()


# Fare range
df_excel.Fare.max() - df_excel.Fare.min()








import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import os
import acquire

from sklearn.impute import SimpleImputer
from sklearn.model_selection import train_test_split
from scipy import stats
from env import get_connection














iris_df = acquire.get_iris_data()


iris_df.info() 


iris_df.head()


i_df = iris_df.drop(columns=['species_id', 'species_id.1', 'measurement_id'])
i_df.head()


i_df = i_df.rename(columns={'species_name' : 'species'})
i_df.head()





'''
To create dummy variables for the 'species' column in the Iris dataset 
and concatenate them onto the existing dataframe, we can use the pandas 
library in Python. 
'''
import pandas as pd
from sklearn.datasets import load_iris

# Load the Iris dataset
iris = load_iris()
iris_df = pd.DataFrame(data=iris.data, columns=iris.feature_names)

# Add the 'species' column to the dataframe
iris_df['species'] = iris.target_names[iris.target]

# Create dummy variables for the 'species' column
species_dummies = pd.get_dummies(iris_df['species'], prefix='species')

# Concatenate the dummy variables onto the original dataframe
iris_df = pd.concat([iris_df, species_dummies], axis=1)

# Display the modified dataframe
print(iris_df.head())





def prep_iris():

    iris_df = acquire.get_iris_data()

    i_df = iris_df.drop(columns=['species_id', 'species_id.1', 'measurement_id'])

    i_df = i_df.rename(columns={'species_name' : 'species'})

    return i_df
    


prep_iris().head()


i_df = prep_iris()








titanic_data = acquire.get_titanic_data()
titanic_data.info()


titanic_data.isna().sum() # brief statement tells me how many nulls i have and in which column


round(titanic_data.isna().sum() /len(titanic_data) * 100, 2)


titanic_data.head()


t_data = titanic_data.drop(columns= ['embarked', 'parch'])


round(titanic_data.isna().sum() /len(titanic_data) * 100, 2)


def prep_titanic():

    titanic_data = acquire.get_titanic_data()
    
    t_data = titanic_data.drop(columns= ['embarked'])

    return t_data


t_data.info()


prep_titanic().head()


t_df = prep_titanic()








telco_data = acquire.get_telco_data()
telco_data.info()


telco_data.isna().sum() # brief statement tells me how many nulls i have and in which column


te_data = telco_data.drop(columns=['customer_id', 'partner', 'payment_type_id', 'contract_type_id', 'internet_service_type_id'])
te_data.head()


def prep_telco():

    telco_data = acquire.get_telco_data()

    te_data = telco_data.drop(columns=['customer_id', 'payment_type_id', 'contract_type_id', 'internet_service_type_id'])

    return te_data


prep_telco().head()


te_df = prep_telco()


te_df.info()


te_df.internet_service_type.unique








# this function will be good for splitting classification data per our target

def train_val_test(df, strat, seed = 42):

    train, val_test = train_test_split(df, train_size = 0.7,
                                       random_state = seed,
                                       stratify = df[strat])

    val, test = train_test_split(val_test, train_size = 0.5,
                                 random_state = seed,
                                 stratify = val_test[strat])

    return train, val, test


train_iris, validate_iris, test_iris = train_val_test(i_df, strat='species')


# Validate my split

print(f'train_iris -> {train_iris.shape}')
print(f'validate_iris -> {validate_iris.shape}')
print(f'test_iris -> {test_iris.shape}')


train_titanic, validate_titanic, test_titanic = train_val_test(t_df, strat='survived')


# Validate my split

print(f'train_titanic -> {train_titanic.shape}')
print(f'validate_titanic -> {validate_titanic.shape}')
print(f'test_titanic -> {test_titanic.shape}')


train_telco, validate_telco, test_telco = train_val_test(te_df, strat='churn')


# Validate my split

print(f'train_telco -> {train_telco.shape}')
print(f'validate_telco -> {validate_telco.shape}')
print(f'test_telco -> {test_telco.shape}')








# 20% test, 80% train_validate
# then of the 80% train_validate: 30% validate, 70% train.

train, test = train_test_split(i_df, test_size=.2, random_state=123, stratify=i_df.species)
train, validate = train_test_split(train, test_size=.3, random_state=123, stratify=train.species)



# Validate my split.

print(f'train -> {train.shape}')
print(f'validate -> {validate.shape}')
print(f'test -> {test.shape}')


def split_data(df):
    '''
    take in a DataFrame and return train, validate, and test DataFrames; stratify on survived.
    return train, validate, test DataFrames.
    '''
    train_validate, test = train_test_split(df, test_size=.2, random_state=123, stratify=df.species)
    train, validate = train_test_split(train_validate, 
                                       test_size=.3, 
                                       random_state=123, 
                                       stratify=train_validate.species)
    
    return train, validate, test


train_i, validate_i, test_i = split_data(i_df)


# Validate my split

print(f'train_iris -> {train_iris.shape}')
print(f'validate_iris -> {validate_iris.shape}')
print(f'test_iris -> {test_iris.shape}')








# 20% test, 80% train_validate
# then of the 80% train_validate: 30% validate, 70% train.

train, test = train_test_split(t_df, test_size=.2, random_state=123, stratify=t_df.survived)
train, validate = train_test_split(train, test_size=.3, random_state=123, stratify=train.survived)



# Validate my split.

print(f'train_titanic -> {train.shape}')
print(f'validate_titanic -> {validate.shape}')
print(f'test_titanic -> {test.shape}')


def split_data(df):
    '''
    take in a DataFrame and return train, validate, and test DataFrames; stratify on survived.
    return train, validate, test DataFrames.
    '''
    train_validate, test = train_test_split(t_df, test_size=.2, random_state=123, stratify=t_df.survived)
    train, validate = train_test_split(train_validate, 
                                       test_size=.3, 
                                       random_state=123, 
                                       stratify=train_validate.survived)
    
    return train, validate, test


train_t, validate_t, test_t = split_data(t_df)


# Validate my split.

print(f'train_titanic -> {train.shape}')
print(f'validate_titanic -> {validate.shape}')
print(f'test_titanic -> {test.shape}')








# 20% test, 80% train_validate
# then of the 80% train_validate: 30% validate, 70% train.

train, test = train_test_split(te_df, test_size=.2, random_state=123, stratify=te_df.churn)
train, validate = train_test_split(train, test_size=.3, random_state=123, stratify=train.churn)



# Validate my split.

print(f'train_telco -> {train.shape}')
print(f'validate_telco -> {validate.shape}')
print(f'test_telco -> {test.shape}')


def split_data(df):
    '''
    take in a DataFrame and return train, validate, and test DataFrames; stratify on survived.
    return train, validate, test DataFrames.
    '''
    train_validate, test = train_test_split(te_df, test_size=.2, random_state=123, stratify=te_df.churn)
    train, validate = train_test_split(train_validate, 
                                       test_size=.3, 
                                       random_state=123, 
                                       stratify=train_validate.churn)
    
    return train, validate, test


train_te, validate_te, test_te = split_data(te_df)


# Validate my split.

print(f'train_telco -> {train.shape}')
print(f'validate_telco -> {validate.shape}')
print(f'test_telco -> {test.shape}')















































# imports/modules
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from acquire import get_iris_data

from sklearn.model_selection import train_test_split
from scipy import stats


def iris_pipeline():
    
    df = prep_iris()
    
    train, val, test = train_val_test(df, 'species')

    # should add one in the future:
    # train, val, test = impute_vals(train, val, test)
    
    return train, val, test


train, val, test = iris_pipeline()





train.head()


sns.histplot(data = train.sepal_length)
plt.show()


sns.histplot(data = train.sepal_width)
plt.show()


sns.histplot(data = train.sepal_width)
plt.show()


sns.histplot(data = train.petal_length)
plt.show()


sns.histplot(data = train.petal_width)
plt.show()


sns.boxplot(data = train.sepal_length)
plt.show()


sns.boxplot(data = train.sepal_width)
plt.show()


sns.boxplot(data = train.petal_length)
plt.show()


sns.boxplot(data = train.petal_width)
plt.show()


train.describe().T


sns.countplot(data = train, x='species')
plt.show()








plt.title("Sepal Length by Species")
sns.barplot(x="species", y="sepal_length", data=train)
sl_mean = train.sepal_length.mean()
plt.axhline(sl_mean, label="Overall Mean", color='magenta', linestyle='--')
plt.legend()
plt.show()


plt.title("Sepal Width by Species")
sns.barplot(x="species", y="sepal_width", data=train)
sw_mean = train.sepal_width.mean()
plt.axhline(sw_mean, label="Overall Mean", color='magenta', linestyle='--')
plt.legend()
plt.show()


plt.title("Petal Length by Species")
sns.barplot(x="species", y="petal_length", data=train)
pl_mean = train.petal_width.mean()
plt.axhline(pl_mean, label="Overall Mean", color='magenta', linestyle='--')
plt.legend()
plt.show()


plt.title("Petal Width by Species")
sns.barplot(x="species", y="petal_width", data=train)
pw_mean = train.petal_width.mean()
plt.axhline(pw_mean, label="Overall Mean", color='magenta', linestyle='--')
plt.legend()
plt.show()


pw_mean = sns.barplot(data=train, x = train.species, y = train.petal_width)

overall_mean = train.petal_width.mean()

pw_mean.axhline(y=overall_mean, color='magenta', linestyle='--', label='Overall Mean')

pw_mean.set(xlabel='Species', ylabel='Petal Width', title='Petal Width by Species')

pw_mean.legend()
plt.show()


train.groupby('species').describe().T


def eval_p(p, a = 0.05):

    if p < a:

        print(f'The result is significant, we reject the null hypothesis with a p-value of {round(p, 2)}.')

    else:

        print(f'We failed to reject the null hypothesis with a p-value of {round(p, 2)}.')


virginica_sl = train[train['species'] == 'virginica'].sepal_length
virginica_sw = train[train['species'] == 'virginica'].sepal_width
virginica_pl = train[train['species'] == 'virginica'].petal_length
virginica_pw =  train[train['species'] == 'virginica'].petal_width


virginica_sl.head()


virginica_sw.head()


virginica_pl.head()


virginica_pw.head()


versicolor_sl = train[train['species'] == 'versicolor'].sepal_length
versicolor_sw = train[train['species'] == 'versicolor'].sepal_width
versicolor_pl = train[train['species'] == 'versicolor'].petal_width
versicolor_pw = train[train['species'] == 'versicolor'].petal_length


static, p = stats.mannwhitneyu(virginica_pw, versicolor_pw)
p


eval_p(p)


static, p = stats.mannwhitneyu(virginica_pl, versicolor_pl)
p


eval_p(p)


static, p = stats.mannwhitneyu(virginica_sw, versicolor_sw)
p


eval_p(p)


static, p = stats.mannwhitneyu(virginica_sl, versicolor_sl)
p


eval_p(p)








sns.pairplot(data = train, hue='species')
plt.show()


sns.relplot(x="sepal_length", y="petal_length", hue="species", data=train)
plt.show()


train.corr()








train.head()


train = train[train['species'] != 'versicolor']


sns.boxplot(data = train, x = 'petal_length', y = 'species' )
plt.show()


virginica = train[train['species'] == 'virginica'].petal_length
virginica.head()


setosa = train[train['species'] == 'setosa'].petal_length
setosa.head()





virginica.var()


setosa.var()





t, p = stats.ttest_ind(virginica, setosa, equal_var=False)
t, p


eval_p(p)














def titanic_pipeline():
    
    df = prep_titanic()
    
    train, val, test = train_val_test(df, 'survived')

    # should add one in the future:
    # train, val, test = impute_vals(train, val, test)
    
    return train, val, test


train, val, test = titanic_pipeline()


train.head()


# Analyzing categorical features chosen below
plt.figure(figsize=(12, 6))

plt.subplot(131)
sns.barplot(x='sex', y='survived', data=train, errorbar=None)
plt.subplot(132)
sns.barplot(x='pclass', y='survived', data=train, errorbar=None)
plt.subplot(133)
sns.barplot(x='embark_town', y='survived', data=train, errorbar=None)

plt.tight_layout()
plt.show()


# Analyzing numerical features chosen below
plt.figure(figsize=(12, 6))

plt.subplot(121)
sns.histplot(data=titanic_data, x='age', hue='survived', multiple='stack', bins=20)
plt.subplot(122)
sns.histplot(data=titanic_data, x='fare', hue='survived', multiple='stack', bins=20)

plt.tight_layout()
plt.show()


# looking at the columns to see what I can drop
train.columns


train = train.drop(columns=['passenger_id', 'deck', 'class', 'pclass'])


train.head()


# Bin age
train['ageBin'] = pd.cut(train['age'], bins=[0, 18, 35, 50, 100], labels=['0-18', '19-35', '36-50', '50+'])

# Bin fare
train['fareBin'] = pd.cut(train['fare'], bins=[0, 50, 100, 1000], labels=['0-50', '51-100', '100+'])


train.head()


# Creating a new feature "FamilySize"
train['family_size'] = train['sibsp'] + train['parch']
train['family_size']


train.head()


# Analyzing numerical features chosen below
plt.figure(figsize=(12, 6))

plt.subplot(121)
sns.histplot(data=train, x='family_size', hue='survived', multiple='stack').set(title='Survived Family Sizes')
plt.subplot(122)
sns.histplot(data=train, x='family_size', hue='embark_town', multiple='stack').set(title='Family Size by Embark town')

plt.tight_layout()
plt.show()











def telco_pipeline():
    
    df = prep_telco()
    
    train, val, test = train_val_test(df, 'churn')

    # should add one in the future:
    # train, val, test = impute_vals(train, val, test)
    
    return train, val, test


train, val, test = telco_pipeline()


# Analyzing categorical features chosen below
plt.figure(figsize=(12, 6))

plt.subplot(131)
sns.histplot(data=train, x='senior_citizen', hue='churn').set(title= 'Senior Citizens Churned vs. Not Churned')
plt.subplot(132)
sns.histplot(data=train, x='partner', hue='churn').set(title= 'Married Churned vs. Not Churned')
plt.subplot(133)
sns.histplot(data=train, x='dependents', hue='churn').set(title= 'With Dependents Churned vs. Not Churned')

plt.tight_layout()
plt.show()


train.columns.tolist()


# Decided to drop tenure feature
train = train.drop(columns='tenure')
train.head()


train['monthly_charges'] = train.monthly_charges.round()


train['total_charges'] = pd.to_numeric(train['total_charges'], errors='coerce')
train['total_charges'] = train['total_charges'].round()


# Analyzing numerical features chosen below
plt.figure(figsize=(12, 6))

plt.subplot(121)
sns.histplot(data=train, x='monthly_charges',  hue='churn', multiple='stack', bins=20)
plt.subplot(122)
sns.histplot(data=train, x='total_charges', hue='churn', multiple='stack', bins=20)

plt.tight_layout()
plt.show()






